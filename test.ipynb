{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File already exists.\n"]}],"source":["import os\n","import urllib.request\n","\n","\n","def download_file(file_link, filename):\n","    # Checks if the file already exists before downloading\n","    if not os.path.isfile(filename):\n","        urllib.request.urlretrieve(file_link, filename)\n","        print(\"File downloaded successfully.\")\n","    else:\n","        print(\"File already exists.\")\n","\n","# Dowloading GGML model from HuggingFace\n","ggml_model_path = \"https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf\"\n","\n","filename = \"input/mistral-7b-v0.1.Q4_K_M.gguf\"\n","\n","download_file(ggml_model_path, filename)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"]}],"source":["from llama_cpp import Llama\n","\n","llm = Llama(model_path=filename, n_ctx=512, n_batch=126)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["def get_llm_generator(\n","    prompt,\n","    max_tokens=512,\n","    temperature=0,\n","    top_p=0.9,\n","    echo=False,\n","    stop=[\"[INST]\"],\n","):\n","    text_generator = llm(\n","        prompt,\n","        max_tokens=max_tokens,\n","        temperature=temperature,\n","        top_p=top_p,\n","        echo=echo,\n","        stream=True,\n","        stop=stop,\n","    )\n","    return text_generator\n","\n","def get_llm_generation(\n","    prompt,\n","    max_tokens=512,\n","    temperature=0,\n","    top_p=0.9,\n","    echo=False,\n","    stop=[\"[INST]\"],\n","):\n","    text_generation = llm(\n","        prompt,\n","        max_tokens=max_tokens,\n","        temperature=temperature,\n","        top_p=top_p,\n","        echo=echo,\n","        stop=stop,\n","    )\n","    return text_generation[\"choices\"][0][\"text\"].strip()\n","\n","def generate_chat_prompt(input):\n","    system = \"You are a helpful bot that answers any questions the user may have. Only answer in short clear sentences.\"\n","    chat_prompt_template = f\"<s>[INST] {system} [/INST]</s>{input}\"\n","    return chat_prompt_template\n","\n","def generate_skill_extraction_prompt(input):\n","    system = \"You are a helpful bot that extracts a couple of skills the user will learn after learning what the user wants to learn. Only answer in a list of comma separated words in between curly brackets.\"\n","    chat_prompt_template = f\"<s>[INST] {system} [/INST]</s>{input}\"\n","    return chat_prompt_template\n","\n","def process_skill_extraction_prompt(input, prompt):\n","    if \"{\" in input and \"}\" in input:\n","        return input[input.find(\"{\")+1:input.find(\"}\")]\n","    else:\n","        return prompt"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["  You\n","can learn more\n","about AI by\n","reading books ,\n","taking courses ,\n","and attending con\n","ferences . There\n","are many resources\n","available online and\n","off line to\n","help you learn\n","more about AI\n",".  \n"]}],"source":["# Test User Help Generator\n","prompt = generate_chat_prompt(\n","    \"How can I learn more about AI?\"\n",")\n","\n","generator = get_llm_generator(prompt)\n","\n","buffer = []\n","buffer_size = 3\n","\n","for word in generator:\n","    buffer.append(word[\"choices\"][0][\"text\"].strip())\n","    if len(buffer) >= buffer_size:\n","        print(\" \".join(buffer))\n","        buffer.clear()"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"data":{"text/plain":["'English, Spanish, French, German, Italian, Portuguese, Russian, Chinese, Japanese, Korean'"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["# Test Skill Extraction\n","prompt = \"I want to learn the most important languages in the world.\"\n","processed_prompt = generate_skill_extraction_prompt(\n","    prompt\n",")\n","output = get_llm_generation(processed_prompt)\n","processed_output = process_skill_extraction_prompt(output, prompt)\n","processed_output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"imec","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
